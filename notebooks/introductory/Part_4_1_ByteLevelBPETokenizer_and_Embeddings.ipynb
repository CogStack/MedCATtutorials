{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lXXf2emwSdpN",
    "outputId": "0c7b8593-5f1a-45ae-bbea-d2ad265301af"
   },
   "outputs": [],
   "source": [
    "# Install medcat\n",
    "! pip install medcat==1.2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jj679NkSfsS"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFwGQA3tSzF4"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mR1Hqh5wSPaD",
    "outputId": "896ca473-cf06-4ded-855c-51f976242ef9"
   },
   "outputs": [],
   "source": [
    "!mkdir ./data\n",
    "!mkdir ./models\n",
    "!wget https://raw.githubusercontent.com/CogStack/MedCATtutorials/main/notebooks/introductory/data/noteevents.csv -P ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDBI5vy2VQRH"
   },
   "source": [
    "### Meta Annotations with MedCAT\n",
    "\n",
    "To train meta-annotations (e.g. Experiencer, Negation...) we need two additional models:\n",
    "- Tokenizer: to tokenize the text\n",
    "- Embeddings: Word2Vec or any other type of embeddings that will be used for meta annotations. \n",
    "\n",
    "For meta-annotations we will use a custom BiLSTM model with simulated attention that works very well with sub-word tokenizers and embeddings creating using Word2Vec or BERT (for simplicity we will use w2v here). All of this is also available for download (check next tutorial) and we only need to rebuild the tokenizer/embeddings if our use-case is from a very specific domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "K0ihb0tLSlRu",
    "outputId": "6e6ea0a0-8882-4b45-e473-1e2bba42fdaa"
   },
   "outputs": [],
   "source": [
    "# To train the tokenizer we will use all the data we have from our dummy dataset.\n",
    "df = pd.read_csv(DATA_DIR + \"noteevents.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBtVgjCZS0gF"
   },
   "outputs": [],
   "source": [
    "# The tokenizers from huggingface require us to save all the text used for \n",
    "#training into one/multiple text files.\n",
    "f = open(DATA_DIR + \"tok_data.txt\", 'w')\n",
    "for text in df['text'].values:\n",
    "  #We'll remove new lines, so that we have one document in one line\n",
    "  text = text.strip().replace(\"\\n\", ' ')\n",
    "  f.write(text.lower()) # Lowercase text to remove noise\n",
    "  f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dukwUnN1TPCg"
   },
   "outputs": [],
   "source": [
    "# Create, train and save the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(DATA_DIR + \"tok_data.txt\")\n",
    "tokenizer.save(\"./models/bbpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mh9uCi6ETiH3"
   },
   "outputs": [],
   "source": [
    "# Now we tokenize all the text we have and train word2vec\n",
    "f = open(DATA_DIR + \"tok_data.txt\", 'r')\n",
    "# Note that if you have a very large dataset, use iterators that\n",
    "#read the text line by line from the file, do not load the whole file\n",
    "#into memory.\n",
    "data = []\n",
    "for line in f:\n",
    "  data.append(tokenizer.encode(line).tokens)\n",
    "w2v = Word2Vec(data, vector_size=300, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMhwMcGI7Oqz",
    "outputId": "32032617-ce7f-4afc-b3c6-bfde04af8d62"
   },
   "outputs": [],
   "source": [
    "# Check is word2vec trained, Ġ - for this tokenizer denotes start of word (a space)\n",
    "w2v.wv.most_similar('Ġcancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrRr4Pd_UgvY"
   },
   "outputs": [],
   "source": [
    "# Now we just have to create the embeddings matrix\n",
    "embeddings = []\n",
    "for i in range(tokenizer.get_vocab_size()):\n",
    "  word = tokenizer.id_to_token(i)\n",
    "  if word in w2v.wv:\n",
    "    embeddings.append(w2v.wv[word])\n",
    "  else:\n",
    "    # Assign a random vector if the word was not frequent enough to receive\n",
    "    #an embedding\n",
    "    embeddings.append(np.random.rand(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz465LmIVD2E"
   },
   "outputs": [],
   "source": [
    "# Save the embeddings\n",
    "np.save(open(\"./models/embeddings.npy\", 'wb'), np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmrmJkDq4lT7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MedCAT Tutorial | Part 4.1 - ByteLevelBPETokenizer and Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
