{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXXf2emwSdpN"
   },
   "outputs": [],
   "source": [
    "# Install medcat\n",
    "! pip install medcat==1.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jj679NkSfsS"
   },
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFwGQA3tSzF4"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mR1Hqh5wSPaD",
    "outputId": "ccd6b965-11df-4e27-d721-a55f5807ccd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-24 20:08:30--  https://raw.githubusercontent.com/CogStack/MedCAT/master/tutorial/data/noteevents.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7171226 (6.8M) [text/plain]\n",
      "Saving to: ‘./data/noteevents.csv’\n",
      "\n",
      "noteevents.csv      100%[===================>]   6.84M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2021-10-24 20:08:30 (84.4 MB/s) - ‘./data/noteevents.csv’ saved [7171226/7171226]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./data\n",
    "!mkdir ./models\n",
    "!wget https://raw.githubusercontent.com/CogStack/MedCATtutorials/main/notebooks/introductory/data/noteevents.csv -P ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDBI5vy2VQRH"
   },
   "source": [
    "### Meta Annotations with MedCAT\n",
    "\n",
    "To train meta-annotations (e.g. Experiencer, Negation...) we need two additional models:\n",
    "- Tokenizer: to tokenize the text\n",
    "- Embeddings: Word2Vec or any other type of embeddings that will be used for meta annotations. \n",
    "\n",
    "For meta-annotations we will use a custom BiLSTM model with simulated attention that works very well with sub-word tokenizers and embeddings creating using Word2Vec or BERT (for simplicity we will use w2v here). All of this is also available for download (check next tutorial) and we only need to rebuild the tokenizer/embeddings if our use-case is from a very specific domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "K0ihb0tLSlRu",
    "outputId": "91b9a3ac-b1c4-435b-aedb-14a48aa4ac6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>chartdate</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2086</td>\n",
       "      <td>Urology</td>\n",
       "      <td>CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2086</td>\n",
       "      <td>Emergency Room Reports</td>\n",
       "      <td>CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2086</td>\n",
       "      <td>General Medicine</td>\n",
       "      <td>CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2086</td>\n",
       "      <td>General Medicine</td>\n",
       "      <td>CHIEF COMPLAINT:,  Followup on hypertension an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2086</td>\n",
       "      <td>Consult - History and Phy.</td>\n",
       "      <td>CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                               text\n",
       "0           0  ...  CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...\n",
       "1           1  ...  CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...\n",
       "2           2  ...  CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...\n",
       "3           3  ...  CHIEF COMPLAINT:,  Followup on hypertension an...\n",
       "4           4  ...  CHIEF COMPLAINT: , Blood in urine.,HISTORY OF ...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To train the tokenizer we will use all the data we have from our dummy dataset.\n",
    "df = pd.read_csv(DATA_DIR + \"noteevents.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBtVgjCZS0gF"
   },
   "outputs": [],
   "source": [
    "# The tokenizers from huggingface require us to save all the text used for \n",
    "#training into one/multiple text files.\n",
    "f = open(DATA_DIR + \"tok_data.txt\", 'w')\n",
    "for text in df['text'].values:\n",
    "  #We'll remove new lines, so that we have one document in one line\n",
    "  text = text.strip().replace(\"\\n\", ' ')\n",
    "  f.write(text.lower()) # Lowercase text to remove noise\n",
    "  f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dukwUnN1TPCg"
   },
   "outputs": [],
   "source": [
    "# Create, train and save the tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(DATA_DIR + \"tok_data.txt\")\n",
    "tokenizer.save(\"./models/bbpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mh9uCi6ETiH3"
   },
   "outputs": [],
   "source": [
    "# Now we tokenize all the text we have and train word2vec\n",
    "f = open(DATA_DIR + \"tok_data.txt\", 'r')\n",
    "# Note that if you have a very large dataset, use iterators that\n",
    "#read the text line by line from the file, do not load the whole file\n",
    "#into memory.\n",
    "data = []\n",
    "for line in f:\n",
    "  data.append(tokenizer.encode(line).tokens)\n",
    "w2v = Word2Vec(data, vector_size=300, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMhwMcGI7Oqz",
    "outputId": "9990e0a2-9211-48a4-82b8-29efe948b220"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ġcolon', 0.7221731543540955),\n",
       " ('Ġcarcinoma', 0.7145481705665588),\n",
       " ('Ġmetastatic', 0.6969555616378784),\n",
       " ('Ġmesothelioma', 0.6846612095832825),\n",
       " ('Ġfather', 0.6737938523292542),\n",
       " ('Ġdied', 0.6339669823646545),\n",
       " ('Ġmother', 0.6332935094833374),\n",
       " ('Ġlung', 0.6187140345573425),\n",
       " ('Ġbreast', 0.6179347038269043),\n",
       " ('Ġfamily', 0.6172316074371338)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check is word2vec trained, Ġ - for this tokenizer denotes start of word (a space)\n",
    "w2v.wv.most_similar('Ġcancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrRr4Pd_UgvY"
   },
   "outputs": [],
   "source": [
    "# Now we just have to create the embeddings matrix\n",
    "embeddings = []\n",
    "for i in range(tokenizer.get_vocab_size()):\n",
    "  word = tokenizer.id_to_token(i)\n",
    "  if word in w2v.wv:\n",
    "    embeddings.append(w2v.wv[word])\n",
    "  else:\n",
    "    # Assign a random vector if the word was not frequent enough to receive\n",
    "    #an embedding\n",
    "    embeddings.append(np.random.rand(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hz465LmIVD2E"
   },
   "outputs": [],
   "source": [
    "# Save the embeddings\n",
    "np.save(open(\"./models/embeddings.npy\", 'wb'), np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmrmJkDq4lT7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MedCAT Tutorial | Part 4.1 - ByteLevelBPETokenizer and Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
