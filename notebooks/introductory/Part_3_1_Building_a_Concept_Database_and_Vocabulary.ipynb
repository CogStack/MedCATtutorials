{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_j_Gu7s3wTO"
   },
   "source": [
    "# Let's build and initialise a MedCAT model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4bQfWfXlKWJ"
   },
   "source": [
    "### First we need to install MedCAT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlMQ2iQrlG69",
    "outputId": "51bd1abf-2ae6-4301-82e0-ab837ecade57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting medcat==1.2.0\n",
      "  Downloading medcat-1.2.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 5.4 MB/s \n",
      "\u001b[?25hCollecting torch~=1.8.1\n",
      "  Downloading torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 804.1 MB 2.7 kB/s \n",
      "\u001b[?25hCollecting tqdm<4.62.3,>=4.27\n",
      "  Downloading tqdm-4.62.2-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 4.4 MB/s \n",
      "\u001b[?25hCollecting elasticsearch~=7.10\n",
      "  Downloading elasticsearch-7.15.1-py2.py3-none-any.whl (378 kB)\n",
      "\u001b[K     |████████████████████████████████| 378 kB 59.7 MB/s \n",
      "\u001b[?25hCollecting datasets~=1.14.0\n",
      "  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
      "\u001b[K     |████████████████████████████████| 290 kB 45.6 MB/s \n",
      "\u001b[?25hCollecting spacy<3.1.4,>=3.0.1\n",
      "  Downloading spacy-3.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 41.1 MB/s \n",
      "\u001b[?25hCollecting transformers~=4.11.3\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 20.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: dill~=0.3.4 in /usr/local/lib/python3.7/dist-packages (from medcat==1.2.0) (0.3.4)\n",
      "Requirement already satisfied: sklearn~=0.0 in /usr/local/lib/python3.7/dist-packages (from medcat==1.2.0) (0.0)\n",
      "Requirement already satisfied: numpy<1.21.0,>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from medcat==1.2.0) (1.19.5)\n",
      "Collecting gensim~=4.1.2\n",
      "  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.1 MB 2.7 kB/s \n",
      "\u001b[?25hCollecting jsonpickle~=2.0.0\n",
      "  Downloading jsonpickle-2.0.0-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pandas<=1.3.4,>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from medcat==1.2.0) (1.1.5)\n",
      "Collecting scipy<=1.7.1,>=1.5.4\n",
      "  Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.5 MB 39 kB/s \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets~=1.14.0->medcat==1.2.0) (2.23.0)\n",
      "Collecting huggingface-hub<0.1.0,>=0.0.19\n",
      "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.3 MB/s \n",
      "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 46.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets~=1.14.0->medcat==1.2.0) (21.0)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets~=1.14.0->medcat==1.2.0) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets~=1.14.0->medcat==1.2.0) (0.70.12.2)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 59.4 MB/s \n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 48.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets~=1.14.0->medcat==1.2.0) (4.8.1)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch~=7.10->medcat==1.2.0) (1.24.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch~=7.10->medcat==1.2.0) (2021.5.30)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim~=4.1.2->medcat==1.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets~=1.14.0->medcat==1.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets~=1.14.0->medcat==1.2.0) (3.3.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets~=1.14.0->medcat==1.2.0) (3.13)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets~=1.14.0->medcat==1.2.0) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.4,>=1.1.5->medcat==1.2.0) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<=1.3.4,>=1.1.5->medcat==1.2.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas<=1.3.4,>=1.1.5->medcat==1.2.0) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets~=1.14.0->medcat==1.2.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets~=1.14.0->medcat==1.2.0) (2.10)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn~=0.0->medcat==1.2.0) (0.22.2.post1)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.0-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
      "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.4,>=3.0.1->medcat==1.2.0) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.4,>=3.0.1->medcat==1.2.0) (3.0.5)\n",
      "Collecting thinc<8.1.0,>=8.0.9\n",
      "  Downloading thinc-8.0.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (627 kB)\n",
      "\u001b[K     |████████████████████████████████| 627 kB 60.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.4,>=3.0.1->medcat==1.2.0) (0.4.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 44.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.4,>=3.0.1->medcat==1.2.0) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.4,>=3.0.1->medcat==1.2.0) (57.4.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.4,>=3.0.1->medcat==1.2.0) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.4,>=3.0.1->medcat==1.2.0) (0.8.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
      "\u001b[K     |████████████████████████████████| 451 kB 51.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.1.4,>=3.0.1->medcat==1.2.0) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers~=4.11.3->medcat==1.2.0) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 55.4 MB/s \n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 50.7 MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 21.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.1.4,>=3.0.1->medcat==1.2.0) (7.1.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets~=1.14.0->medcat==1.2.0) (21.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 61.0 MB/s \n",
      "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 56.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.4,>=3.0.1->medcat==1.2.0) (2.0.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers~=4.11.3->medcat==1.2.0) (1.0.1)\n",
      "Installing collected packages: multidict, yarl, catalogue, async-timeout, typer, tqdm, srsly, scipy, pyyaml, pydantic, fsspec, aiohttp, xxhash, tokenizers, thinc, spacy-legacy, sacremoses, pathy, huggingface-hub, transformers, torch, spacy, jsonpickle, gensim, elasticsearch, datasets, medcat\n",
      "  Attempting uninstall: catalogue\n",
      "    Found existing installation: catalogue 1.0.0\n",
      "    Uninstalling catalogue-1.0.0:\n",
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.3\n",
      "    Uninstalling tqdm-4.62.3:\n",
      "      Successfully uninstalled tqdm-4.62.3\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0+cu111\n",
      "    Uninstalling torch-1.9.0+cu111:\n",
      "      Successfully uninstalled torch-1.9.0+cu111\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 2.2.4\n",
      "    Uninstalling spacy-2.2.4:\n",
      "      Successfully uninstalled spacy-2.2.4\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 3.6.0\n",
      "    Uninstalling gensim-3.6.0:\n",
      "      Successfully uninstalled gensim-3.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.10.0+cu111 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\n",
      "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.1 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 catalogue-2.0.6 datasets-1.14.0 elasticsearch-7.15.1 fsspec-2021.10.1 gensim-4.1.2 huggingface-hub-0.0.19 jsonpickle-2.0.0 medcat-1.2.0 multidict-5.2.0 pathy-0.6.0 pydantic-1.8.2 pyyaml-6.0 sacremoses-0.0.46 scipy-1.7.1 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.2 thinc-8.0.11 tokenizers-0.10.3 torch-1.8.1 tqdm-4.62.2 transformers-4.11.3 typer-0.4.0 xxhash-2.0.2 yarl-1.7.0\n",
      "Collecting en-core-web-md==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.1.0/en_core_web_md-3.1.0-py3-none-any.whl (45.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 45.4 MB 16 kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.1.0) (3.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (57.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (21.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.4.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (8.0.11)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.23.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.19.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (4.62.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-md==3.1.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# Install MedCAT\n",
    "! pip install medcat==1.2.3\n",
    "# Get the scispacy model\n",
    "! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWScf8BW0BpY"
   },
   "source": [
    "**Restart the runtime if on colab, sometimes necessary after installing models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KByaUPYNk7gk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from medcat.vocab import Vocab\n",
    "from medcat.cdb import CDB\n",
    "from medcat.config import Config\n",
    "from medcat.cdb_maker import CDBMaker\n",
    "from medcat.cat import CAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKgZTiZxk7gp"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IRLcv4dGIEbS",
    "outputId": "c1b3e8c8-0d19-4cf5-e46b-34f5720bc1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-25 11:09:58--  https://raw.githubusercontent.com/CogStack/MedCAT/develop/tutorial/data/cdb_simple.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 50 [text/plain]\n",
      "Saving to: ‘./data/cdb_simple.csv’\n",
      "\n",
      "cdb_simple.csv      100%[===================>]      50  --.-KB/s    in 0s      \n",
      "\n",
      "2021-10-25 11:09:59 (2.12 MB/s) - ‘./data/cdb_simple.csv’ saved [50/50]\n",
      "\n",
      "--2021-10-25 11:09:59--  https://raw.githubusercontent.com/CogStack/MedCAT/develop/tutorial/data/cdb_advanced.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 150 [text/plain]\n",
      "Saving to: ‘./data/cdb_advanced.csv’\n",
      "\n",
      "cdb_advanced.csv    100%[===================>]     150  --.-KB/s    in 0s      \n",
      "\n",
      "2021-10-25 11:09:59 (5.12 MB/s) - ‘./data/cdb_advanced.csv’ saved [150/150]\n",
      "\n",
      "--2021-10-25 11:09:59--  https://raw.githubusercontent.com/CogStack/MedCAT/master/tutorial/data/vocab_data.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 72 [text/plain]\n",
      "Saving to: ‘./data/vocab_data.txt’\n",
      "\n",
      "vocab_data.txt      100%[===================>]      72  --.-KB/s    in 0s      \n",
      "\n",
      "2021-10-25 11:09:59 (3.80 MB/s) - ‘./data/vocab_data.txt’ saved [72/72]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load files if in google colab, otherwise skip this step\n",
    "!wget https://raw.githubusercontent.com/CogStack/MedCATtutorials/main/notebooks/introductory/data/cdb_simple.csv -P ./data/\n",
    "!wget https://raw.githubusercontent.com/CogStack/MedCATtutorials/main/notebooks/introductory/data/cdb_advanced.csv -P ./data/\n",
    "!wget https://raw.githubusercontent.com/CogStack/MedCATtutorials/main/notebooks/introductory/data/vocab_data.txt -P ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kj24ZU79D-xE"
   },
   "source": [
    "# MedCAT Components\n",
    "The medcat model requires 3 model components to run.\n",
    "1. Vocab\n",
    "2. CDB\n",
    "3. Config (cdb configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9POZ_dwsk7gu"
   },
   "source": [
    "## Building a Vocabulary\n",
    "\n",
    "The first of the two required models when running MedCAT is a Vocabulary model (Vocab). The model is used for two things: (1) Spell checking; and (2) Word Embedding. \n",
    "\n",
    "The Vocab is very simple and you can easily build it from a file that is structured as below:\n",
    "```\n",
    "<token>\\t<word_count>\\t<vector_embedding_separated_by_spaces>\n",
    "```\n",
    "`token` - Usually a word or subword if you are using Byte Pair Encoding or something similar.\n",
    "\n",
    "`word_count` - The count for this word in your dataset or in any large dataset (wikipedia also works nicely).\n",
    "\n",
    "`vector_embedding_separated_by_spaces` - precalculated vector embedding, can be from Word2Vec or BERT\n",
    "\n",
    "---\n",
    "An example with 3-dimension embedding would be:\n",
    "```\n",
    "house\t34444\t 0.3232 0.123213 1.231231\n",
    "dog\t14444\t0.76762 0.76767 1.45454\n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "The file is basically a TSV, but should not have any heading. \n",
    "\n",
    "---\n",
    "\n",
    "**NOTE**: If spelling is important for your use-case, take care that there are no misspelt words in the Vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgMSGHyhk7gv"
   },
   "outputs": [],
   "source": [
    "# Let's have a look at an example, I've created a small vocabulary with only 2 words (the ones from above)\n",
    "# Let's try to create a vocabulary from this two words.\n",
    "\n",
    "vocab = Vocab()\n",
    "vocab.add_words(DATA_DIR +'vocab_data.txt', replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPl6ghXUk7gy"
   },
   "source": [
    "**And that is everything, with this we have built our vocab and no futher training is necessary.**\n",
    "\n",
    "---\n",
    "\n",
    "A couple of useful functions of the vocab are presented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HeVMPs5Zk7gy",
    "outputId": "980983ca-4c9f-4218-9b8d-53c0291c614c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['house', 'dog'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the words in the vocab\n",
    "vocab.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TofkHoHo0XKU",
    "outputId": "83d49708-6289-47e8-af1d-ca2b42b8dc35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog': {'cnt': 14444, 'ind': 1, 'vec': array([0.76762, 0.76767, 1.45454])},\n",
       " 'house': {'cnt': 34444,\n",
       "  'ind': 0,\n",
       "  'vec': array([0.3232  , 0.123213, 1.231231])}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SclVqlDgk7g2",
    "outputId": "1ed6e4cb-b8b9-4d7f-a4e7-d4f6472d87dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['house', 'dog', 'test'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to add words manually (one by one) use:\n",
    "vocab.add_word(\"test\", cnt=31, vec=[1.42, 1.44, 1.55], replace=True)\n",
    "vocab.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnCgpKrEk7g5",
    "outputId": "45ade622-1a31-47c7-b0bc-65295ca6fcd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3232  , 0.123213, 1.231231])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get a vector of word use:\n",
    "vocab.vec(\"house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bO4IEvrJk7g8",
    "outputId": "ea1677da-531a-48b5-acf0-dbc55968f96b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34444"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or to get the count\n",
    "vocab['house']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLg63Z9Yk7g-",
    "outputId": "f6a91a99-7073-4039-93f3-753e727c7810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check if a word is in the vocab:\n",
    "\"house\" in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG3FCinSl_Sq"
   },
   "source": [
    "### Before we save the vocab model, we need to create the unigram table for negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdC8eennmSM4"
   },
   "outputs": [],
   "source": [
    "# This is necessary after each change of the vocabulary (when we add new words)\n",
    "vocab.make_unigram_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6itJcEXk7hA"
   },
   "source": [
    "### Save the Vocab model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgRtW7eqk7hB"
   },
   "outputs": [],
   "source": [
    "vocab.save(DATA_DIR + \"vocab.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YBbwcNUk7hD"
   },
   "source": [
    "### Load the Vocab model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_tOsE6ak7hE"
   },
   "outputs": [],
   "source": [
    "vocab = Vocab.load(DATA_DIR + \"vocab.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptRmHln9k7hG"
   },
   "source": [
    "## Building the Concept Database (CDB)\n",
    "\n",
    "The second model we are going to need when using MedCAT is the Concept Database (CDB). This database holds a list of all concepts that we would like to detect and link to. For a lot of medical use-cases we would use giant databases like the UMLS or SNOMED CT. However, MedCAT can be used with any database no matter how big/small it is. \n",
    "\n",
    "To prepare the CDB we start off with a CSV with the following structure:\n",
    "```\n",
    "cui,name\n",
    "1,kidney failure\n",
    "7,CoVid 2\n",
    "7,coronavirus\n",
    "```\n",
    "This is the most basic version of the CSV file, it has only:\n",
    "\n",
    "`cui` - The concept unique identifier, this is simply an `ID` in your database.\n",
    "\n",
    "`name` - String/Name of that concept. It is important to write all possible names and abbreviations for a concept of interest.\n",
    "\n",
    "If you have a concept that can be recognised through multiple different names (like the one above with cui=7), you can simply add multiple rows with the same concept ID and MedCAT will merge that during the build phase.\n",
    "\n",
    "## The Full CSV Specification\n",
    "```\n",
    "cui,name,ontologies,name_status,type_ids,description\n",
    "1,Kidney Failure,SNOMED,P,T047,kidneys stop working\n",
    ".\n",
    ".\n",
    ".\n",
    "```\n",
    "The rest of the fields are optional, each can be included or left out in your CSV:\n",
    "\n",
    "`ontologies` - Source ontology, e.g. HPO, SNOMED, HPC,...\n",
    "\n",
    "`name_status` - Term type e.g. P - Primary Name. Primary names are important and I would always recommend to add this fields when creating your CDB. This will help distinguish between synonyms.\n",
    "\n",
    "`type_ids` - Type Ids are the broad category in which a concept may fall under. This is used to rapidly filter for concepts which fall under a specific category. In UMLS this could be the Semantic type identifier - e.g. T047 (taken from UMLS). A list of all semantic types can be found [here](https://metamap.nlm.nih.gov/Docs/SemanticTypes_2018AB.txt).\n",
    "In SNOMED one could use the Semantic tags. E.g (Disease). A list of all Snomed semantic tags can be found [here](https://confluence.ihtsdotools.org/display/DOCGLOSS/semantic+tag).\n",
    "\n",
    "\n",
    "`description` - Description of this concept\n",
    "\n",
    "***Note***: If one concept has multiple names, you can also separate the different names by a \"|\" - pipe - symbol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5IR-xFO4oph"
   },
   "outputs": [],
   "source": [
    "cdb_simple = pd.read_csv(DATA_DIR + 'cdb_simple.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "tHr26s3s4vJN",
    "outputId": "a24687f5-c8ec-4ad3-abda-0f8ed91db6b0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cui</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kidney failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>CoVid 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>coronavirus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cui            name\n",
       "0    1  kidney failure\n",
       "1    7         CoVid 2\n",
       "2    7     coronavirus"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdb_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rasu5PajojYZ"
   },
   "source": [
    "Let's try building our own concept databse from a simple CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y75WVeca08z3"
   },
   "outputs": [],
   "source": [
    "# First initialise the default configuration\n",
    "config = Config()\n",
    "config.general['spacy_model'] = 'en_core_web_md'\n",
    "maker = CDBMaker(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "op5-LDOfk7hH",
    "outputId": "ef0a9396-e9ab-4bf8-d28b-c871ea66448e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Started importing concepts from: ./data/cdb_advanced.csv\n",
      "Current progress: 0% at 0.000s per 0 rows\n",
      "Current progress: 50% at 0.027s per 0 rows\n",
      "Started importing concepts from: ./data/cdb_simple.csv\n",
      "Current progress: 0% at 0.000s per 0 rows\n",
      "Current progress: 33% at 0.013s per 0 rows\n",
      "Current progress: 67% at 0.012s per 0 rows\n"
     ]
    }
   ],
   "source": [
    "# Create an array containing CSV files that will be used to build our CDB\n",
    "csv_path = [ DATA_DIR + 'cdb_advanced.csv', DATA_DIR + 'cdb_simple.csv',]\n",
    "\n",
    "# Create your CDB\n",
    "cdb = maker.prepare_csvs(csv_path, full_build=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08agsFBnk7hQ"
   },
   "source": [
    "**That is all, nothing else is necessary to build the CDB**\n",
    "\n",
    "---\n",
    "\n",
    "Some useful functions of the cdb are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwlOBilek7hJ",
    "outputId": "54c0938d-6da0-4a6a-9988-fa80e8dc1f08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kidney~failure': ['1'], 'failure~of~kidneys': ['1'], 'failure~of~kidney': ['1'], 'kf': ['1'], 'k~.~failure': ['1'], 'covid~2': ['7'], 'coronavirus': ['7']}\n"
     ]
    }
   ],
   "source": [
    "# To display all names and cui in the db\n",
    "print(cdb.name2cuis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uD3AYqqkk7hQ",
    "outputId": "dad125d5-8191-4127-8259-ecee1d64f060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'kf', 'k~.~failure', 'kidney~failure', 'failure~of~kidneys', 'failure~of~kidney'}, '7': {'coronavirus', 'covid~2'}}\n"
     ]
    }
   ],
   "source": [
    "# To display all unique cuis and corresponding names in the db \n",
    "print(cdb.cui2names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWTRmz_duRT2",
    "outputId": "c120a7ad-c475-4fb7-bf46-4cb8d34b58c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'Kidney Failure'}\n"
     ]
    }
   ],
   "source": [
    "# To display cui to preferred name\n",
    "print(cdb.cui2preferred_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j18tyQ2Kk7hV",
    "outputId": "1f5534ab-a937-4ddb-e0c9-0dea275318be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'T047'}, '7': set()}\n"
     ]
    }
   ],
   "source": [
    "# We have a link from cui to type ids\n",
    "print(cdb.cui2type_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lpx7zGvwk7ha"
   },
   "source": [
    "### Save the Concept Database model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fC01maOzk7ha"
   },
   "outputs": [],
   "source": [
    "cdb.save(DATA_DIR + \"cdb.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97uiDwvAk7hc"
   },
   "source": [
    "### Load the Concept Database model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9Ft8PbFk7hc"
   },
   "outputs": [],
   "source": [
    "cdb = CDB.load(DATA_DIR + \"cdb.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xqmmAue-UE4"
   },
   "source": [
    "## Setting the CDB configuration\n",
    "\n",
    "The CDB config sets the model parameters.\n",
    "This allows you to tailor the model to your own specific use case. Although the default configuration will suit the majority of use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eH1xiXvG-aWR"
   },
   "outputs": [],
   "source": [
    "# For further information on the cdb configuration options and explore what the default options are.\n",
    "??cdb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aLoW6ltAa6l"
   },
   "outputs": [],
   "source": [
    "# Set a couple of parameters, they are usually set via environments, but\n",
    "#here we will do it explicitly. You can read more about each option in the \n",
    "#medcat repository: https://github.com/CogStack/MedCAT\n",
    "\n",
    "cdb.config.ner['min_name_len'] = 2\n",
    "cdb.config.ner['upper_case_limit_len'] = 3\n",
    "cdb.config.general['spell_check'] = True\n",
    "cdb.config.linking['train_count_threshold'] = 10\n",
    "cdb.config.linking['similarity_threshold'] = 0.3\n",
    "cdb.config.linking['train'] = True\n",
    "cdb.config.linking['disamb_length_limit'] = 5\n",
    "cdb.config.general['full_unlink'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqmVITvWCIr6"
   },
   "source": [
    "Note: Don't forget to save the cdb with the new configurations if you want to reuse them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZvhmkIL8433"
   },
   "source": [
    "# Create a MedCAT model pack\n",
    "\n",
    "A MedCAT model pack is an easy way to store all the various components of a MedCAT model in one place.\n",
    "\n",
    "This includes the CDB, CDB configurations, Vocab, and even various MetaCAT models! We will learn more about the latter in the following tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxcW2GcM-c-M"
   },
   "outputs": [],
   "source": [
    "# Initialise the model\n",
    "cat = CAT(cdb=cdb, config=cdb.config, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cXusFuI-drw",
    "outputId": "e8f7a473-dbd9-4e49-cfb8-3ec79ffb917e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This will save all models into a zip file, can take some time and require quite a bit of disk space.\n"
     ]
    }
   ],
   "source": [
    "# Create and save a model pack\n",
    "cat.create_model_pack(DATA_DIR + \"my_first_medcat_modelpack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fwiKys4k7he"
   },
   "source": [
    "# End\n",
    "\n",
    "This is everything you need to create your own MedCAT models. In the following tutorials you will learn how to uses modelpacks to train models and use them to annotate documents. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MedCAT Tutorial | Part 3.1 Building a Concept Database and Vocabulary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
